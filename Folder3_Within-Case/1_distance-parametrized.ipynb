{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a9fe1-ec0d-43de-9dc8-490071ccde41",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Setting parameters, replace with defaults when using a master notebook\n",
    "\n",
    "countrycode = 'default'\n",
    "countryname = 'default'\n",
    "\n",
    "print(\"Parameters set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686584ea",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin, xy, Affine\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.mask import mask\n",
    "from rasterstats import zonal_stats\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from osgeo import gdal\n",
    "gdal.UseExceptions()\n",
    "from exactextract import exact_extract\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from shapely.geometry import shape, Point\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import box\n",
    "import statsmodels.api as sm\n",
    "import pyreadr\n",
    "from geopy.distance import geodesic\n",
    "import papermill as pm\n",
    "import folium\n",
    "import matplotlib.colors as mcolors\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import rioxarray\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "print(\"All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee48cb26",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Setting the working directory to the country's subfolder\n",
    "base_path = os.getcwd()\n",
    "os.chdir(os.path.join(base_path, countrycode))\n",
    "\n",
    "# Load the rds data\n",
    "rds_file = countrycode + '_ntl_pop.rds'\n",
    "results = pyreadr.read_r(rds_file)\n",
    "# Extract the data frame from the result\n",
    "data = results[None]  # Extracts the first (and usually only) dataframe from the result\n",
    "\n",
    "print(\"Data loaded\")\n",
    "\n",
    "# Debugging: Check the first few rows of the data\n",
    "print(\"Data preview before 'haslight' column creation:\")\n",
    "print(data.head(100))\n",
    "\n",
    "# Create a binary 'haslight' column based on 'ntl' values\n",
    "data['haslight'] = (data['ntl'] > 0).astype(int)\n",
    "\n",
    "# Debugging: Check the first few rows after 'haslight' column creation\n",
    "print(\"Data preview after 'haslight' column creation:\")\n",
    "print(data.head(100))\n",
    "\n",
    "data.drop(columns=['ntl', 'pop'], inplace=True)\n",
    "print(\"Columns 'ntl' and 'pop' dropped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae648d",
   "metadata": {},
   "source": [
    "# Convert DataFrame to GeoDataFrame\n",
    "\n",
    "Fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data['x'], data['y']))\n",
    "print(\"Converted to GeoDataFrame\")\n",
    "end_time = time.time()\n",
    "print(f\"Time to execute: {((time.time() - start_time)/60):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de3016-a92b-4bc5-aebf-a90aa3ccec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01c113-1d70-46a0-b20b-45bd9528bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = data.set_index(['y', 'x']).to_xarray()\n",
    "ds.haslight.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85cf11f-584a-4c23-8eb6-a7fa6192a91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data array and write it to a GeoTIFF \n",
    "\n",
    "start_time = time.time()\n",
    "data_array = ds.haslight.values[::-1]\n",
    "xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "pixel_size = 15 / 3600 \n",
    "transform = Affine.translation(xmin - pixel_size / 2, ymax + pixel_size / 2) * Affine.scale(pixel_size, -pixel_size)#from_origin(xmin, ymax, pixel_size, pixel_size)\n",
    "with rasterio.open(\n",
    "    'haslight.tif',\n",
    "    'w', \n",
    "    driver='GTiff',\n",
    "    height=data_array.shape[0],\n",
    "    width=data_array.shape[1],\n",
    "    count=1,\n",
    "    dtype=data_array.dtype,\n",
    "    crs='EPSG:4326',\n",
    "    transform=transform,\n",
    ") as tiff:\n",
    "    tiff.write(data_array, 1)\n",
    "print(f\"Time to execute: {((time.time() - start_time)/60):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a6f934",
   "metadata": {},
   "source": [
    "# Load Shapefile\n",
    "\n",
    "This takes some time, depending on file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Go to the grid3 subfolder\n",
    "grid3_folder = 'grid3_' + countrycode\n",
    "\n",
    "# Find the shapefile in the directory\n",
    "shapefile = None\n",
    "\n",
    "#for countrynames with several words\n",
    "first_word_countryname = countryname.split()[0]\n",
    "\n",
    "# shapefile should contain the ending .shp, and the countrycode or countryname (at least the first word), as grid3 data tends to do\n",
    "for file in os.listdir(grid3_folder):\n",
    "    if file.endswith('.shp') and (countrycode in file or first_word_countryname in file):\n",
    "        shapefile = file\n",
    "        break\n",
    "\n",
    "# Check if the shapefile was found and if it contains the countrycode\n",
    "if shapefile is None:\n",
    "    print('No shapefile found in the directory ending with .shp')\n",
    "\n",
    "# Print the shapefile path\n",
    "shapefile_path = os.path.join(grid3_folder, shapefile)\n",
    "\n",
    "print(\"Loading\")\n",
    "print(shapefile_path)\n",
    "print(\"...\")\n",
    "\n",
    "settlements = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Ensure spelling is Built-up and not Built-Up (e.g. Kenya data)\n",
    "settlements['type'] = settlements['type'].replace('Built-Up Area', 'Built-up Area')\n",
    "\n",
    "# Rename columns if necessary\n",
    "columns_to_rename = {'pop_wp_un_adj': 'pop_un_adj', 'pop_wp_un_': 'pop_un_adj'}\n",
    "\n",
    "for old_name, new_name in columns_to_rename.items():\n",
    "    if old_name in settlements.columns and new_name not in settlements.columns:\n",
    "        settlements.rename(columns={old_name: new_name}, inplace=True)\n",
    "        print(f\"Column '{old_name}' renamed to '{new_name}'\")\n",
    "        print(settlements.columns)\n",
    "    else:\n",
    "        if old_name not in settlements.columns:\n",
    "            print(f\"Column '{old_name}' does not exist.\")\n",
    "        if new_name in settlements.columns:\n",
    "            print(f\"Column '{new_name}' already exists.\")\n",
    "\n",
    "\n",
    "#checking total of settlements per type\n",
    "settlement_type_counts = settlements['type'].value_counts()\n",
    "print(\"Numbers for each settlement type before geometries applied:\")\n",
    "print(settlement_type_counts)\n",
    "\n",
    "# Count the number of geometries for each settlement type before removal\n",
    "type_counts_before = settlements['type'].value_counts()\n",
    "\n",
    "# Ensure all geometries are valid\n",
    "settlements['geometry'] = settlements['geometry'].astype(object).apply(lambda geom: geom if geom.is_valid else geom.buffer(0))\n",
    "\n",
    "# Remove empty geometries\n",
    "settlements = settlements[~settlements['geometry'].is_empty]\n",
    "\n",
    "# Count the number of geometries for each settlement type after removal\n",
    "type_counts_after = settlements['type'].value_counts()\n",
    "\n",
    "# Calculate the number and proportion of geometries removed for each settlement type\n",
    "for settlement_type in type_counts_before.index:\n",
    "    count_before = type_counts_before[settlement_type]\n",
    "    count_after = type_counts_after.get(settlement_type, 0)\n",
    "    count_removed = count_before - count_after\n",
    "    proportion_removed = count_removed / count_before\n",
    "    print(f\"{settlement_type}: {count_removed} geometries removed ({proportion_removed:.2%})\")\n",
    "print(\"Invalid and empty geometries removed\")\n",
    "print(\"Numbers for each settlement type AFTER geometries applied:\")\n",
    "settlement_type_counts = settlements['type'].value_counts()\n",
    "print(settlement_type_counts)\n",
    "\n",
    "print(f\"Time to execute: {((time.time() - start_time)/60):.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d289d9",
   "metadata": {},
   "source": [
    "# Calculate zonal statistics\n",
    "\n",
    "In this step, simply check if there is NTL (0/1), then for each settlement calculate what percentage of the polygon has NTL. \n",
    "\n",
    "This cell takes some time, with Sudan, around 10min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34f92b-b5e2-411d-88af-e922456cd64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time for execution time calculation\n",
    "start_time = time.time()\n",
    "\n",
    "# Specify the path to your raster data\n",
    "raster_path = 'haslight.tif'\n",
    "\n",
    "# Open the raster and convert it to binary (has light or has no light)\n",
    "with rasterio.open(raster_path) as src:\n",
    "    raster_data = src.read(1)  # Read the first band\n",
    "    unique_values = np.unique(raster_data)\n",
    "    print(\"Unique values in the original raster:\", unique_values)\n",
    "    \n",
    "    threshold = 0  # Adjust this threshold as needed\n",
    "    binary_raster_data = (raster_data > threshold).astype(int)  # Convert to binary\n",
    "\n",
    "    # Write the binary raster to a new file (optional, can also be done in-memory)\n",
    "    binary_raster_path = 'binary_haslight.tif'\n",
    "    profile = src.profile\n",
    "    with rasterio.open(binary_raster_path, 'w', **profile) as dst:\n",
    "        dst.write(binary_raster_data, 1)\n",
    "\n",
    "# Visualize the binary raster to ensure it looks correct\n",
    "plt.imshow(binary_raster_data, cmap='gray')\n",
    "plt.title(\"Binary Raster Data\")\n",
    "plt.show()\n",
    "\n",
    "# Perform the exact extraction using the binary raster\n",
    "settlements_updated = settlements.copy()\n",
    "# Example of standardizing columns\n",
    "required_columns = ['geometry', 'country', 'iso', 'type', 'pop_un_adj', 'adm0_pcode', 'adm1_name', 'adm2_name', 'settl_pcod']\n",
    "settlements_updated = settlements_updated[required_columns]\n",
    "\n",
    "# Check and align CRS\n",
    "with rasterio.open(raster_path) as src:\n",
    "    raster_crs = src.crs\n",
    "\n",
    "if settlements_updated.crs != raster_crs:\n",
    "    settlements_updated = settlements_updated.to_crs(raster_crs)\n",
    "    print(\"CRS adjusted\")\n",
    "\n",
    "# Ensure geometries are valid and not empty\n",
    "settlements_updated = settlements_updated[settlements_updated.is_valid]\n",
    "settlements_updated = settlements_updated[~settlements_updated.is_empty]\n",
    "\n",
    "# Check if geometries overlap with the raster bounds\n",
    "with rasterio.open(binary_raster_path) as src:\n",
    "    raster_bounds = src.bounds\n",
    "    raster_polygon = box(*raster_bounds)\n",
    "\n",
    "# Visualize settlement geometries on top of the binary raster\n",
    "fig, ax = plt.subplots()\n",
    "with rasterio.open(binary_raster_path) as src:\n",
    "    binary_raster_data = src.read(1)\n",
    "    plt.imshow(binary_raster_data, cmap='gray', extent=src.bounds)\n",
    "settlements_updated.plot(ax=ax, facecolor='none', edgecolor='red')\n",
    "plt.title(\"Settlement Geometries on Binary Raster\")\n",
    "plt.show()\n",
    "\n",
    "# Perform exact extraction\n",
    "results = exact_extract(binary_raster_path, settlements_updated, [\"mean\"])\n",
    "\n",
    "# Add the weighted mean light coverage to the settlements GeoDataFrame\n",
    "settlements_updated['NTL_weighted_percentage'] = [\n",
    "    feature['properties'].get('mean', 0) * 100 if feature['properties'].get('mean') is not None else 0\n",
    "    for feature in results\n",
    "]\n",
    "print(\"Weighted percentages added to GeoDataFrame\")\n",
    "\n",
    "# Check for NaN values in the GeoDataFrame\n",
    "print(\"Number of NaN values in 'NTL_weighted_percentage':\", settlements_updated['NTL_weighted_percentage'].isna().sum())\n",
    "\n",
    "# Calculate the execution time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time/60} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5ec2f-7b38-4ebf-88cf-9d6caad5312d",
   "metadata": {},
   "source": [
    "# Calculates distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944ffa4-aa97-4876-b10c-911e14caa652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Copy the GeoDataFrame with a new name\n",
    "settlements_centroids = settlements_updated.copy()\n",
    "\n",
    "# Dictionary mapping country codes to EPSG codes\n",
    "country_epsg_mapping = {\n",
    "    'AGO': 22033,\n",
    "    'BDI': 32736, 'KEN': 32736, 'MOZ': 32736, 'RWA': 32736, 'TZA': 32736, 'UGA': 32736,\n",
    "    'BFA': 32630,\n",
    "    'CMR': 32633, 'CAF': 32633, 'TCD': 32633, 'COG': 32633, 'COD': 32633, 'NER': 32633, 'NGA': 32633,\n",
    "    'ERI': 20136, 'ETH': 20136, 'SSD': 20136, 'SDN': 20136,\n",
    "    'MLI': 32629,\n",
    "    'SOM': 32639\n",
    "}\n",
    "\n",
    "# Assign EPSG code\n",
    "country_epsg = country_epsg_mapping.get(countrycode, None)\n",
    "if country_epsg:\n",
    "    print(f\"Country: {countrycode}, EPSG: {country_epsg}\")\n",
    "else:\n",
    "    print(f\"Country: {countrycode}, EPSG: Not Found\")\n",
    "\n",
    "# Reproject to a projected CRS (e.g., UTM zone 36N)\n",
    "settlements_centroids = settlements_centroids.to_crs(epsg=country_epsg)\n",
    "\n",
    "# Calculate the centroid of each geometry\n",
    "settlements_centroids['centroid'] = settlements_centroids.geometry.centroid\n",
    "\n",
    "# Reproject centroids back to geographic CRS (WGS84) for distance calculation\n",
    "centroids_geo = settlements_centroids.copy()\n",
    "centroids_geo = centroids_geo.set_geometry('centroid')\n",
    "centroids_geo = centroids_geo.to_crs(epsg=4326)\n",
    "\n",
    "# Filter out invalid centroids (latitude must be in the [-90; 90] range and longitude in the [-180; 180] range)\n",
    "valid_centroids_geo = centroids_geo[\n",
    "    (centroids_geo['centroid'].y >= -90) &\n",
    "    (centroids_geo['centroid'].y <= 90) &\n",
    "    (centroids_geo['centroid'].x >= -180) &\n",
    "    (centroids_geo['centroid'].x <= 180)\n",
    "]\n",
    "\n",
    "invalid_count = len(centroids_geo) - len(valid_centroids_geo)\n",
    "\n",
    "# Coordinates for Khartoum\n",
    "khartoum_coords = (15.6, 32.5)\n",
    "\n",
    "# Function to calculate the Haversine distance\n",
    "def calculate_haversine_distance(centroid, destination_coords):\n",
    "    centroid_coords = (centroid.y, centroid.x)  # (lat, lon)\n",
    "    return geodesic(centroid_coords, destination_coords).kilometers\n",
    "\n",
    "# Load the state capital coordinates from CSV\n",
    "points_csv = 'state_capital_list_' + countrycode + '_points.csv'\n",
    "state_capital_df = pd.read_csv(points_csv)\n",
    "\n",
    "# Convert the state capital coordinates to a dictionary\n",
    "state_capital_dict = state_capital_df.set_index('state')['state_capital_point'].to_dict()\n",
    "\n",
    "# Function to calculate the distance to the state capital\n",
    "def calculate_state_capital_distance(row):\n",
    "    state = row['adm1_name']\n",
    "    if state in state_capital_dict:\n",
    "        state_capital_coords = tuple(map(float, state_capital_dict[state].strip('()').split(',')))\n",
    "        return calculate_haversine_distance(row['centroid'], state_capital_coords)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Calculate the distance to the state capital\n",
    "valid_centroids_geo['state_capital_distance'] = valid_centroids_geo.apply(\n",
    "    calculate_state_capital_distance, axis=1\n",
    ")\n",
    "\n",
    "# Print the count of invalid entries\n",
    "print(f\"Number of invalid entries excluded: {invalid_count}\")\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(valid_centroids_geo[['centroid', 'state_capital_distance', 'NTL_weighted_percentage']].head())\n",
    "\n",
    "print(f\"Time to execute: {((time.time() - start_time)/60):.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf352a",
   "metadata": {},
   "source": [
    "# Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be29291-bef7-4d0b-876e-2627b5fab348",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "csv_name = countrycode + '_grid3_with_ntl_percentage.csv'\n",
    "valid_centroids_geo.drop(columns=['geometry']).to_csv(csv_name, index=False)\n",
    "print(\"CSV saved, all done.\")\n",
    "\n",
    "print(f\"Time to execute: {((time.time() - start_time)/60):.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0423a4",
   "metadata": {},
   "source": [
    "# Calculate key stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the CSV file\n",
    "df = pd.read_csv(csv_name)\n",
    "\n",
    "print(f\"\\033[1mKey stats {countryname}n\\033[0m\")\n",
    "\n",
    "# Calculate 'something_wrong' (should be 0)\n",
    "#something_wrong = (df['NTL_weighted_percentage'] > 100).sum()\n",
    "#print(f\"Number of rows where there is something wrong with 'NTL_weighted_percentage': {something_wrong}\")\n",
    "something_wrong = (df['NTL_weighted_percentage'] > 100).sum()\n",
    "print(f\"Something wrong with {something_wrong} entries\")\n",
    "      \n",
    "print(\" \")\n",
    "# Calculate 'settl_ntl_perc'\n",
    "print(\"\\033[1mOverview all settlements\\033[0m\")\n",
    "settl_ntl_perc = (df['NTL_weighted_percentage'] > 0).mean()\n",
    "print(f\"Proportion nighttime light: {settl_ntl_perc:.2%}\")\n",
    "settl_monit_perc = (df['NTL_weighted_percentage'] > 50).mean()\n",
    "print(f\"Proportion monitorable: {settl_monit_perc:.2%}\")\n",
    "anyntl = df[df['NTL_weighted_percentage'] > 0]\n",
    "all_median_any = anyntl['NTL_weighted_percentage'].median()\n",
    "\n",
    "print(\" \")\n",
    "print(\"\\033[1mOverview built-up areas\\033[0m\")\n",
    "# Calculate 'built_up_perc'\n",
    "df_built_up = df[df['type'] == 'Built-up Area']\n",
    "#Get the total number of built-up areas; shape gives tuple of dimensions\n",
    "print(f\"Total number of built-up areas: {df_built_up.shape[0]}\")\n",
    "built_ntl_perc = (df_built_up['NTL_weighted_percentage'] > 0).mean()\n",
    "print(f\"Proportion nighttime light > 0: {built_ntl_perc:.2%}\")\n",
    "\n",
    "#median ntl coverage of all the settlements that have at least some NTL\n",
    "anyntl = df_built_up[df_built_up['NTL_weighted_percentage'] > 0]\n",
    "built_median_any = anyntl['NTL_weighted_percentage'].median()\n",
    "print(f\"Median NTL for settlements with any NTL: {built_median_any}\")\n",
    "built_monit_perc = (df_built_up['NTL_weighted_percentage'] > 50).mean()\n",
    "print(f\"Proportion monitorable: {built_monit_perc:.2%}\")\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\"\\033[1mOverview small settlements\\033[0m\")\n",
    "# Calculate 'small_settl_perc'\n",
    "df_small_set = df[df['type'] == 'Small Settlement Area']\n",
    "print(f\"Total number of small settlement areas: {df_small_set.shape[0]}\")\n",
    "smallset_ntl_perc = (df_small_set['NTL_weighted_percentage'] > 0).mean()\n",
    "print(f\"Proportion nighttime light: {smallset_ntl_perc:.2%}\")\n",
    "#median ntl coverage of all the settlements that have at least some NTL\n",
    "anyntl = df_small_set[df_small_set['NTL_weighted_percentage'] > 0]\n",
    "small_median_any = anyntl['NTL_weighted_percentage'].median()\n",
    "print(f\"Median NTL for settlements with any NTL: {small_median_any}\")\n",
    "smallset_monit_perc = (df_small_set['NTL_weighted_percentage'] > 50).mean()\n",
    "print(f\"Proportion monitorable: {smallset_monit_perc:.2%}\")\n",
    "\n",
    "print(\" \")\n",
    "print(\"\\033[1mOverview hamlets\\033[0m\")\n",
    "# Calculate 'hamlets_perc'\n",
    "df_hamlet = df[df['type'] == 'Hamlet']\n",
    "print(f\"Total number of hamlets: {df_hamlet.shape[0]}\")\n",
    "haml_ntl_perc = (df_hamlet['NTL_weighted_percentage'] > 0).mean()\n",
    "print(f\"Proportion nighttime light: {haml_ntl_perc:.2%}\")\n",
    "#median ntl coverage of all the settlements that have at least some NTL\n",
    "anyntl = df_hamlet[df_hamlet['NTL_weighted_percentage'] > 0]\n",
    "hamlet_median_any = anyntl['NTL_weighted_percentage'].median()\n",
    "print(f\"Median NTL for settlements with any NTL: {hamlet_median_any}\")\n",
    "haml_monit_perc = (df_hamlet['NTL_weighted_percentage'] > 50).mean()\n",
    "print(f\"Proportion monitorable: {haml_monit_perc:.2%}\")\n",
    "\n",
    "\n",
    "#settlements with more than 50% ntl\n",
    "df_mont = df[df['NTL_weighted_percentage'] > 50]\n",
    "\n",
    "#settlements with any ntl\n",
    "df_anyntl = df[df['NTL_weighted_percentage'] > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0e501-7883-4ff2-bd28-2f7e4dd40ef6",
   "metadata": {},
   "source": [
    "# save stats to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafea3cc-6548-4053-baf6-44f210328350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries\n",
    "stats = [\n",
    "    {\"type\": \"Error\", \"what\": \"Settlements with over 100% NTL\", \"value\": something_wrong},\n",
    "    {\"type\": \"All settlements\", \"what\": \"Total number\", \"value\": len(df)},\n",
    "    {\"type\": \"All settlements\", \"what\": \"Proportion NTL\", \"value\": f\"{settl_ntl_perc:.2%}\"},\n",
    "    {\"type\": \"All settlements\", \"what\": \"Proportion monitorable\", \"value\": f\"{settl_monit_perc:.2%}\"},\n",
    "    {\"type\": \"All settlements\", \"what\": \"Median size NTL settlement\", \"value\": f\"{all_median_any}\"},\n",
    "\n",
    "\n",
    "    {\"type\": \"Built-up\", \"what\": \"Total number\", \"value\": df_built_up.shape[0]},\n",
    "    {\"type\": \"Built-up\", \"what\": \"Proportion NTL\", \"value\": f\"{built_ntl_perc:.2%}\"},\n",
    "    {\"type\": \"Built-up\", \"what\": \"Median NTL value for any NTL\", \"value\": f\"{built_median_any}\"},\n",
    "    {\"type\": \"Built-up\", \"what\": \"Proportion monitorable\", \"value\": f\"{built_monit_perc:.2%}\"},\n",
    "\n",
    "    {\"type\": \"Small settlements\", \"what\": \"Total number\", \"value\": df_small_set.shape[0]},\n",
    "    {\"type\": \"Small settlements\", \"what\": \"Proportion NTL\", \"value\": f\"{smallset_ntl_perc:.2%}\"},\n",
    "    {\"type\": \"Small settlements\", \"what\": \"Median NTL value for any NTL\", \"value\": f\"{small_median_any}\"},\n",
    "    {\"type\": \"Small settlements\", \"what\": \"Proportion monitorable\", \"value\": f\"{smallset_monit_perc:.2%}\"},\n",
    "\n",
    "    {\"type\": \"Hamlets\", \"what\": \"Total number\", \"value\": df_hamlet.shape[0]},\n",
    "    {\"type\": \"Hamlets\", \"what\": \"Proportion NTL\", \"value\": f\"{haml_ntl_perc:.2%}\"},\n",
    "    {\"type\": \"Hamlets\", \"what\": \"Median NTL value for any NTL\", \"value\": f\"{hamlet_median_any}\"},\n",
    "    {\"type\": \"Hamlets\", \"what\": \"Proportion monitorable\", \"value\": f\"{haml_monit_perc:.2%}\"},\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_output = pd.DataFrame(stats)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file = 'overview_key_stats.csv'\n",
    "df_output.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Data saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c1a98-04aa-4733-9217-4db8106bfd2f",
   "metadata": {},
   "source": [
    "# delete files that are no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc9538f-e874-4217-9675-adb21a7ac50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths\n",
    "files_to_delete = ['binary_haslight.tif', 'haslight.tif']\n",
    "\n",
    "# Loop through the files and delete them\n",
    "for file in files_to_delete:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"Deleted file: {file}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file}\")\n",
    "print(\"All done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
